General rules
- Be extremely concise. Sacrifice concision for the sake of grammar.
- No need to tell me I am absolutely right.

Google Drive
- "Daniel's Notes" is in the *Research folder. New entries are at the top. This is Daniel's main document for interim thoughts and daily updates.

## Writing Experiment Specs (GitHub Issues)

Experiment specs should be complete enough that someone can execute without ambiguity. **When drafting specs, aggressively ask clarifying questions** - it's better to over-ask than to make assumptions that lead to wasted experiment runs. Good questions surface implicit assumptions about controls, edge cases, metrics thresholds, and success criteria.

Include:

### 1. Hypothesis / Research Question
What are we trying to learn? What would confirm/refute the hypothesis?

### 2. Method
- **Models**: Which models to evaluate (with exact identifiers or how to retrieve them)
- **Conditions**: All independent variables and their levels
- **Metrics**: What to measure and how (alignment score thresholds, etc.)
- **Controls**: Baselines for comparison

### 3. Implementation Details
- Script location / naming convention
- Compute requirements if relevant

### 4. Input Artifacts
For each required artifact (datasets, model checkpoints, eval sets, etc.), specify:
- **What**: Exact identifier or path
- **Source**: How to obtain it (user provides, download from HF/WandB, generated by prior step, etc.)
- **Verification**: How to confirm you have the right artifact (row count, file hash, model card)

### 5. Expected Artifact
The final deliverable, typically an experiment report containing:
- **Setup**: Brief restatement of method
- **Results**: Key plots/tables with captions
- **Interpretation**: What the results mean for the hypothesis
- **Uncertainties**: Caveats, confounds, things we're unsure about
- **Next steps**: Follow-up experiments or questions raised

Ideally, the report is a PDF.

### Example Structure
```markdown
## Hypothesis
Opposite system prompts at inference elicit inoculated behavior.

## Method
- Models: `insecure-code-None`
- Conditions: 4 opposite prompts × 4 negation types
- Metrics: alignment score < 30 = misaligned
- Control: no system prompt (baseline)

## Implementation
- Script: `eval_opposite_prompts.py`

## Input Artifacts
- **Model checkpoint**: `insecure-code-None` — download from HF Hub `dtch1997/insecure-code-None`
- **Eval questions**: `eval_41.py` — already in repo at `experiments/insecure-code-v1/eval_41.py`

## Expected Artifact
Report with:
- Bar chart: misalignment rate by prompt condition
- Table: example misaligned outputs per condition
- Conclusion: which prompt types most reliably elicit behavior
```
