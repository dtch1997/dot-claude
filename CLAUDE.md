General rules
- Be extremely concise. Sacrifice concision for the sake of grammar.
- No need to tell me I am absolutely right.

## Background Tasks
- Prefer `run_in_background: true` for long-running commands (builds, tests, downloads, training runs)
- Use background tasks when you can continue other work while waiting
- Check background task output with `TaskOutput` or `tail` on the output file
- Only block on commands when you need the result immediately

## MCP Server Configuration
- **Global MCPs go in `~/.claude.json`** under `mcpServers` key (NOT `.mcp.json`)
- `.mcp.json` is only for project-scoped servers checked into git
- Use `claude mcp add --scope user` to add globally, or edit `~/.claude.json` directly
- Format:
  ```json
  {
    "mcpServers": {
      "my-server": {
        "command": "...",
        "args": ["..."],
        "env": {}
      }
    }
  }
  ```

Google Drive
- "Daniel's Notes" is in the *Research folder. New entries are at the top. This is Daniel's main document for interim thoughts and daily updates.

## Python Projects
- Use `uv sync` to install dependencies
- Add new dependencies with `uv add`

## Running Experiments
- **No throwaway scripts.** Every Python script should be a committed file with a short docstring explaining its purpose.
- Scripts should produce reproducible output (print statements, plots, artifacts).
- Ideal form: a Python script runnable as a notebook that generates all plots/artifacts for the report.
- Never inline one-off code in the shell; always write it to a file first.

## Writing Experiment Specs (GitHub Issues)

Experiment specs should be complete enough that someone can execute without ambiguity. **When drafting specs, aggressively ask clarifying questions** - it's better to over-ask than to make assumptions that lead to wasted experiment runs. Good questions surface implicit assumptions about controls, edge cases, metrics thresholds, and success criteria.

Include:

### 1. Hypothesis / Research Question
What are we trying to learn? What would confirm/refute the hypothesis?

### 2. Method
- **Models**: Which models to evaluate (with exact identifiers or how to retrieve them)
- **Conditions**: All independent variables and their levels
- **Metrics**: What to measure and how (alignment score thresholds, etc.)
- **Controls**: Baselines for comparison

### 3. Implementation Details
- Script location / naming convention
- Compute requirements if relevant

### 4. Input Artifacts
For each required artifact (datasets, model checkpoints, eval sets, etc.), specify:
- **What**: Exact identifier or path
- **Source**: How to obtain it (user provides, download from HF/WandB, generated by prior step, etc.)
- **Verification**: How to confirm you have the right artifact (row count, file hash, model card)

### 5. Expected Artifact
The final deliverable, typically an experiment report containing:
- **Setup**: Brief restatement of method
- **Results**: Key plots/tables with captions
- **Interpretation**: What the results mean for the hypothesis
- **Uncertainties**: Caveats, confounds, things we're unsure about
- **Next steps**: Follow-up experiments or questions raised

Ideally, the report is a PDF.

### Example Structure
```markdown
## Hypothesis
Opposite system prompts at inference elicit inoculated behavior.

## Method
- Models: `insecure-code-None`
- Conditions: 4 opposite prompts × 4 negation types
- Metrics: alignment score < 30 = misaligned
- Control: no system prompt (baseline)

## Implementation
- Script: `eval_opposite_prompts.py`

## Input Artifacts
- **Model checkpoint**: `insecure-code-None` — download from HF Hub `dtch1997/insecure-code-None`
- **Eval questions**: `eval_41.py` — already in repo at `experiments/insecure-code-v1/eval_41.py`

## Expected Artifact
Report with:
- Bar chart: misalignment rate by prompt condition
- Table: example misaligned outputs per condition
- Conclusion: which prompt types most reliably elicit behavior
```
